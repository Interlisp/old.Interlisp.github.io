# robots.txt file to allow crawling of all webpages.
User-agent: *
Disallow:

